<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Zhaowei Li</title>
    <meta name="description" content="The homepage of Zhaowei Li">
    <!-- <link rel="icon" href="logo.jpg"> -->
    
    <link rel="preload" href="assets/css/0.styles.bf6ecb71.css" as="style"><link rel="preload" href="assets/js/app.ef4e7843.js" as="script"><link rel="preload" href="assets/js/2.5b5922e0.js" as="script"><link rel="preload" href="assets/js/8.c1e4c3b9.js" as="script"><link rel="preload" href="assets/js/4.dc64499e.js" as="script"><link rel="preload" href="assets/js/5.d752ec91.js" as="script"><link rel="prefetch" href="assets/js/3.677d4f8f.js"><link rel="prefetch" href="assets/js/6.0a8475de.js"><link rel="prefetch" href="assets/js/7.4f90f5b5.js"><link rel="prefetch" href="assets/js/9.0cc5bf5a.js">
    <link rel="stylesheet" href="assets/css/0.styles.bf6ecb71.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar home-page"><header class="navbar"><div class="sidebar-button"></div> <a href="/" class="home-link router-link-exact-active router-link-active"><!----> <span class="site-name">Zhaowei Li</span></a> <div class="links"><!----> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="profile"><div class="image"><img src="profile.jpg" alt></div> <div class="info"><div class="name">
      Zhaowei Li(李朝伟)
    </div> 
    <div class="bio"><p>Master student @ Fudan University  </p>
      <p>lizhaowei126@gmail.com</div> 
      
      <div class="socials">
      <div><a href="https://github.com/lzw-lzw" target="_blank">GitHub</a></div> &nbsp/&nbsp
      <div><a href="https://scholar.google.com/citations?user=4CygZ0oAAAAJ" target="_blank">Google Scholar</a></div> &nbsp/&nbsp
      <div><a href="https://x.com/lizhaowei126" target="_blank">Twitter</a></div> &nbsp/&nbsp

      </div> 
      <div class="contact"><div title="Contact me" class="email"></div></div></div></div> 

      <!-- <div><a href="https://github.com/0nutation" target="_blank"><img src="icons/github.svg" alt="GitHub" title="GitHub"></a></div>
      <div><a href="https://scholar.google.com/citations?user=ScVbeu0AAAAJ" target="_blank"><img src="icons/google_scholar.svg" alt="Google Scholar" title="Google Scholar"></a></div>
      <div><a href="https://twitter.com/dongzha35524835" target="_blank"><img src="icons/twitter.png" alt="Twitter" title="Twitter"></a></div>
      <div><a href="https://www.linkedin.com/in/dong-zhang-33481520b/" target="_blank"><img src="icons/linkedin.svg" alt="linkedin" title="linkedin"></a></div>
      <div><a href="https://www.zhihu.com/people/nutation" target="_blank"><img src="icons/zhihu.png" alt="Zhihu" title="Zhihu"></a></div>
      </div> 
      <div class="contact"><div title="Contact me" class="email"></div></div></div></div>  -->
      

      <h2 id="about-me"><a href="#about-me" class="header-anchor">#</a> About Me</h2> 
      <p>Hi! I am a second year M.S. student at <a href="https://www.fudan.edu.cn/en/" target="_blank" rel="noopener noreferrer">Fudan University</a>.
       Currently, I am interning at <a href="https://www.bytedance.com/en/" target="_blank" rel="noopener noreferrer">Bytedance</a>.
      <p>My research interest focuses on <strong>Multi-Modal Large Language Models and Multi-Modal Agents</strong>. 
      <p>I expect to graduate with a master's degree in <strong>June 2025</strong>, and I am actively seeking opportunities for a <strong>Ph.D. and employment</strong>. I'm also open to academic collaboration opportunities. Please feel free to contact me by <a href="mailto:lizhaowei126@gmail.com">lizhaowei126@gmail.com</a> if you are interested!</p> 

<h2 id="news"><a href="#news" class="header-anchor">#</a> News</h2> 
<ul>
<li><p><strong>[2024.5]</strong> We released <strong>UnifiedMLLM</strong></a>, a large language model that models multi-modal, multi-tasks in a unified representation. </li>
<li><p><strong>[2024.5]</strong> We released <a href="https://arxiv.org/pdf/2405.13014" target="_blank" rel="noopener noreferrer"><strong>QCRD</strong></a>, a general method to distilling contrastive rationale knowledge from LLMs into small language models. </li>
<li><p><strong>[2024.5]</strong> Our GroundingGPT is accepted to ACL 2024! See you in Thailand! </li>
<li><p><strong>[2024.4]</strong> We released <a href="https://arxiv.org/pdf/2404.05600.pdf" target="_blank" rel="noopener noreferrer"><strong>SpeechAlign</strong></a>, the first to apply RLHF to align speech language models with human preferences! </li>
<li><p><strong>[2024.1]</strong> We released <a href="https://arxiv.org/pdf/2401.06071.pdf" target="_blank" rel="noopener noreferrer"><strong>GroundingGPT</strong></a>, the first end-to-edn multi-modal grounding model. </li>
<li><p><strong>[2024.1]</strong> We released <a href="https://arxiv.org/pdf/2401.03945.pdf" target="_blank" rel="noopener noreferrer"><strong>SpeechAgents</strong></a>, the first multi-modal multi-agent system. </li>

<!-- <li><p><strong>[2024.5]</strong> We released <a href="https://0nutation.github.io/SpeechGPT2.github.io/" target="_blank" rel="noopener noreferrer"><strong>SpeechGPT2</strong></a>, a emotional intelligent end-to-end spoken dialogue LLM. </li>
<li><p><strong>[2024.5]</strong> Three papers accepted to ACL 2024 main conference! </li>
<li><p><strong>[2024.4]</strong> We released <a href="https://arxiv.org/pdf/2404.05600.pdf" target="_blank" rel="noopener noreferrer"><strong>SpeechAlign</strong></a>, the first to apply RLHF to align speech language models with human preferences! </li>
<li><p><strong>[2024.2]</strong> I give a talk about SpeechGPT series works at <a href="https://www.airmeet.com/e/b2157610-cfe7-11ee-93ec-3b2ce56d50d2" target="_blank" rel="noopener noreferrer"><strong>AGI Leap Summit 2024</strong></a> hosted by <a href="https://superagi.com/" target="_blank" rel="noopener noreferrer"><strong>SuperAGI</strong></a>. </li>
<li><p><strong>[2024.2]</strong> We released <a href="https://arxiv.org/pdf/2402.12226.pdf" target="_blank" rel="noopener noreferrer"><strong>AnyGPT</strong></a>, a unified multi-modal LLM for text, image, speech and music! </li>
<li><p><strong>[2024.1]</strong> We released <a href="https://arxiv.org/pdf/2401.13527.pdf" target="_blank" rel="noopener noreferrer"><strong>SpeechGPT-Gen</strong></a>, an 8B speech LLM efficient in semantic and perceptual information modeling. </li>
<li><p><strong>[2024.1]</strong> We proposed <a href="https://arxiv.org/pdf/2401.11206.pdf" target="_blank" rel="noopener noreferrer"><strong>InferAligner</strong></a>, an effective training-free LLM alignment method. </li>
<li><p><strong>[2024.1]</strong> Our SpeechTokenizer accepted to ICLR 2024! See you in Vienna! </li>
<li><p><strong>[2024.1]</strong> We released <a href="https://arxiv.org/pdf/2401.03945.pdf" target="_blank" rel="noopener noreferrer"><strong>SpeechAgents</strong></a>, the first multi-modal multi-agent system. </li>
<li><p><strong>[2023.10]</strong> Two papers accepted to EMNLP 2023! </li>
<li><p><strong>[2023.8]</strong> We released <a href="https://arxiv.org/pdf/2308.16692.pdf" target="_blank" rel="noopener noreferrer"><strong>SpeechTokenizer</strong></a>, a speech tokenizer designed for speech language models. </li>
<li><p><strong>[2023.5]</strong> We released <a href="https://arxiv.org/pdf/2305.11000.pdf" target="_blank" rel="noopener noreferrer"><strong>SpeechGPT</strong></a>, a conversational speech large language model. </li>
<li><p><strong>[2023.5]</strong> One first-author paper accepted to ACL 2023(Findings)! </li>
<li><p><strong>[2022.9]</strong> I joined FudanNLPLab as a master student. </li> -->
</ul> 

 

<h2 id="Representative Publications"><a href="Representative Publications" class="header-anchor">#</a>Research</h2> 
(*: Equal contribution)

<br>

<div class="md-card">
  <div class="card-image"><img src="projects/groundinggpt.jpg" alt></div> 
  <div class="card-content">
    <p><strong>GroundingGPT: Language-Enhanced Multi-modal Grounding Model</strong></p> 
    <p><em><strong>Zhaowei Li</strong>, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, Tao Wang</em></p> 
    <p>
      [<a href="https://arxiv.org/pdf/2401.06071" target="_blank" rel="noopener noreferrer"><strong>ACL 2024</strong></a>] 
      [<a href="https://github.com/lzw-lzw/GroundingGPT" target="_blank" rel="noopener noreferrer">code <img src="https://img.shields.io/github/stars/lzw-lzw/GroundingGPT"></a>]
      [<a href="https://lzw-lzw.github.io/GroundingGPT.github.io/" target="_blank" rel="noopener noreferrer">demo</a>] 
    </p>
    <p>
      GroundingGPT is the first end-to-end large language model that supports multimodal grounding and understanding tasks.
    </p>
  </div>
  </div> 

  <div class="md-card">
    <div class="card-image"><img src="projects/speechalign.png" alt></div> 
    <div class="card-content">
      <p><strong>SpeechAlign: Aligning Speech Generation to Human Preferences</strong></p> 
      <p><em>Dong Zhang<sup>*</sup>, <strong>Zhaowei Li</strong><sup>*</sup>, Shimin Li, Xin Zhang, Pengyu Wang, Yaqian Zhou, Xipeng Qiu</em></p> 
      <p>
        [<a href="https://arxiv.org/pdf/2404.05600.pdf" target="_blank" rel="noopener noreferrer"><strong>Preprint</strong></a>] 
        [<a href="https://github.com/0nutation/SpeechGPT" target="_blank" rel="noopener noreferrer">code <img src="https://img.shields.io/github/stars/0nutation/SpeechGPT"></a>]
        [<a href="https://0nutation.github.io/SpeechAlign.github.io/" target="_blank" rel="noopener noreferrer">demo</a>] 
      </p>
      <p>
        SpeechAlign is the first to applys RLHF to align speech language models with human preferences and proposes an effective iterative self-improvement strategy that converts weak speech language models to stronger ones.
      </p>
    </div>
    </div> 
    
<div class="md-card">
    <div class="card-image"><img src="projects/speechagents.png" alt></div> 
    <div class="card-content">
      <p><strong>SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems</strong></p> 
      <p><em>Dong Zhang, <strong>Zhaowei Li</strong>, Pengyu Wang, Xin Zhang, Yaqian Zhou, Xipeng Qiu</em></p> 
      <p>
        [<a href="https://arxiv.org/pdf/2401.03945.pdf" target="_blank" rel="noopener noreferrer"><strong>Preprint</strong></a>] 
        [<a href="https://github.com/0nutation/SpeechAgents" target="_blank" rel="noopener noreferrer">code <img src="https://img.shields.io/github/stars/0nutation/SpeechAgents"></a>]
        [<a href="https://0nutation.github.io/SpeechAgents.github.io/" target="_blank" rel="noopener noreferrer">demo</a>] 
      </p>
      <p>
        SpeechAgents is the first multi-modal multi-agent systems.
      </p>
    </div>
    </div> 

  


  <div class="md-card">
    <div class="card-image"><img src="projects/qcrd.png" alt></div> 
    <div class="card-content">
      <p><strong>QCRD: Quality-guided Contrastive Rationale Distillation for Large Language Models</strong></p> 
      <p><em>Wei Wang, <strong>Zhaowei Li</strong>, Qi Xu, Yiqing Cai, Hang Song, Qi Qi, Ran Zhou, Zhida Huang, Tao Wang, Li Xiao</em></p> 
      <p>
        [<a href="https://arxiv.org/pdf/2405.13014" target="_blank" rel="noopener noreferrer"><strong>Preprint</strong></a>] 
      </p>
      <p>
        QCRD is a general method to distilling contrastive rationale knowledge from LLMs into small language models.
      </p>
    </div>
    </div> 




<h2 id="Full Publications"><a href="Full Publications" class="header-anchor">#</a>Full Publications</h2> 

<h3 id="2024"><a href="2024" class="header-anchor">#</a>2024</h3> 
<ul>
<li><p><a href="https://arxiv.org/pdf/2401.06071" target="_blank" rel="noopener noreferrer"><strong>GroundingGPT:Language Enhanced Multi-modal Grounding Model</strong></a> <br><strong>Zhaowei Li</strong>, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, Tao Wang. <br>ACL 2024</li>
<li><p><strong>UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model </strong></a> <br><strong>Zhaowei Li</strong>, Wei Wang, YiQing Cai, Xu Qi, Pengyu Wang, Dong Zhang, Hang Song, Botian Jiang, Zhida Huang, Tao Wang. <br>Preprint</li>
<li><p><a href="https://arxiv.org/pdf/2401.06071" target="_blank" rel="noopener noreferrer"><strong>SpeechAlign: Aligning Speech Generation to Human Preferences</strong></a> <br>Dong Zhang<sup>*</sup>, <strong>Zhaowei Li</strong><sup>*</sup>, Shimin Li, Xin Zhang, Pengyu Wang, Yaqian Zhou, Xipeng Qiu. <br>Preprint</li>
<li><p><a href="https://arxiv.org/pdf/2401.03945" target="_blank" rel="noopener noreferrer"><strong>SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems</strong></a> <br>Dong Zhang, <strong>Zhaowei Li</strong>, Pengyu Wang, Xin Zhang, Yaqian Zhou, Xipeng Qiu. <br>Preprint</li>
<li><p><a href="https://arxiv.org/pdf/2401.03945" target="_blank" rel="noopener noreferrer"><strong>QCRD: Quality-guided Contrastive Rationale Distillation for Large Language Models</strong></a> <br>Wei Wang, <strong>Zhaowei Li</strong>, Qi Xu, Yiqing Cai, Hang Song, Qi Qi, Ran Zhou, Zhida Huang, Tao Wang, Li Xiao. <br>Preprint</li>
    
</ul> 


<h2 id="education"><a href="#education" class="header-anchor">#</a> Education</h2> 
<ul>
<li><p><strong>Fudan University</strong> <span style="color:gray;float:right;">Sept 2022 - Jun 2025</span> <br>
  M.S. in Electronic Engineering</p></li>
<li><p><strong>Fudan University</strong> <span style="color:gray;float:right;">Sept 2018 - Jun 2022</span> <br>
  B.S. in Electronic Engineering</p></li>
</ul>  


<h2 id="internship"><a href="#internship" class="header-anchor">#</a> Internship</h2> 
<ul>
<li><p><strong>Bytedance E-commerce </strong> <span style="color:gray;float:right;">Jul 2023 - Now</span> <br>
  Research on multi-modal large language model</p></li>
</ul>  



<!-- <table
style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
  <tr>
    <td style="padding:20px;width:30%;vertical-align:middle">
      <script type='text/javascript' id='clustrmaps'
        src=''></script>
    </td>
  </tr>
</tbody>
</table> -->

   
      <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="assets/js/app.ef4e7843.js" defer></script><script src="assets/js/2.5b5922e0.js" defer></script><script src="assets/js/8.c1e4c3b9.js" defer></script><script src="assets/js/4.dc64499e.js" defer></script><script src="assets/js/5.d752ec91.js" defer></script>
  </body>
</html>
